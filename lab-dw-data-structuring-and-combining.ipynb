{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25d7736c-ba17-4aff-b6bb-66eba20fbf4e",
   "metadata": {
    "id": "25d7736c-ba17-4aff-b6bb-66eba20fbf4e"
   },
   "source": [
    "# Lab | Data Structuring and Combining Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cdfc70-44c8-478c-81e7-2bc43fdf4986",
   "metadata": {
    "id": "a2cdfc70-44c8-478c-81e7-2bc43fdf4986"
   },
   "source": [
    "## Challenge 1: Combining & Cleaning Data\n",
    "\n",
    "In this challenge, we will be working with the customer data from an insurance company, as we did in the two previous labs. The data can be found here:\n",
    "- https://raw.githubusercontent.com/data-bootcamp-v4/data/main/file1.csv\n",
    "\n",
    "But this time, we got new data, which can be found in the following 2 CSV files located at the links below.\n",
    "\n",
    "- https://raw.githubusercontent.com/data-bootcamp-v4/data/main/file2.csv\n",
    "- https://raw.githubusercontent.com/data-bootcamp-v4/data/main/file3.csv\n",
    "\n",
    "Note that you'll need to clean and format the new data.\n",
    "\n",
    "Observation:\n",
    "- One option is to first combine the three datasets and then apply the cleaning function to the new combined dataset\n",
    "- Another option would be to read the clean file you saved in the previous lab, and just clean the two new files and concatenate the three clean datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "492d06e3-92c7-4105-ac72-536db98d3244",
   "metadata": {
    "id": "492d06e3-92c7-4105-ac72-536db98d3244"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Challenge 1: Combining & Cleaning Data ---\n",
      "\n",
      "[1/5] Reading and Combining Files...\n",
      "   Success! Combined total rows: 12074\n",
      "\n",
      "[2/5] Standardizing Column Names (Snake Case)...\n",
      "   First 5 new columns: ['customer', 'st', 'gender', 'education', 'customer_lifetime_value']\n",
      "\n",
      "[3/5] Cleaning Text Data (Lowercasing & Standardizing)...\n",
      "   Text cleaning complete.\n",
      "\n",
      "[4/5] Converting Numeric Data Types...\n",
      "   Numeric columns dtypes: {'income': dtype('float64'), 'total_claim_amount': dtype('float64')}\n",
      "\n",
      "[5/5] Handling Missing Values (Imputation)...\n",
      "   Imputation complete. Total missing values remaining: 0\n",
      "--- Pipeline Complete! Data is ready for analysis. ---\n",
      "\n",
      "Final Clean Data Sample:\n",
      "  customer          st                                             gender  \\\n",
      "0  rb50392  washington  0        0        nan\\n1          f\\n2        ...   \n",
      "1  qz44356     arizona  0        0        nan\\n1          f\\n2        ...   \n",
      "2  ai49188      nevada  0        0        nan\\n1          f\\n2        ...   \n",
      "3  ww63253  california  0        0        nan\\n1          f\\n2        ...   \n",
      "4  ga49547  washington  0        0        nan\\n1          f\\n2        ...   \n",
      "\n",
      "              education  customer_lifetime_value   income  \\\n",
      "0                master             1.821964e+05      0.0   \n",
      "1              bachelor             6.979536e+05      0.0   \n",
      "2              bachelor             1.288743e+06  48767.0   \n",
      "3              bachelor             7.645862e+05      0.0   \n",
      "4  high school or below             5.363077e+05  36357.0   \n",
      "\n",
      "   monthly_premium_auto  number_of_open_complaints     policy_type  \\\n",
      "0                1000.0                     0.3843   personal auto   \n",
      "1                  94.0                     0.3843   personal auto   \n",
      "2                 108.0                     0.3843   personal auto   \n",
      "3                 106.0                     0.3843  corporate auto   \n",
      "4                  68.0                     0.3843   personal auto   \n",
      "\n",
      "   vehicle_class  total_claim_amount       state  \\\n",
      "0  four-door car            2.704934  california   \n",
      "1  four-door car         1131.464935  california   \n",
      "2   two-door car          566.472247  california   \n",
      "3            suv          529.881344  california   \n",
      "4  four-door car           17.269323  california   \n",
      "\n",
      "                                              gender  \n",
      "0  0        0        nan\\n1        nan\\n2        ...  \n",
      "1  0        0        nan\\n1        nan\\n2        ...  \n",
      "2  0        0        nan\\n1        nan\\n2        ...  \n",
      "3  0        0        nan\\n1        nan\\n2        ...  \n",
      "4  0        0        nan\\n1        nan\\n2        ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# step 1 Reading and Combining Files\n",
    "\n",
    "file1_url = 'https://raw.githubusercontent.com/data-bootcamp-v4/data/main/file1.csv'\n",
    "file2_url = 'https://raw.githubusercontent.com/data-bootcamp-v4/data/main/file2.csv'\n",
    "file3_url = 'https://raw.githubusercontent.com/data-bootcamp-v4/data/main/file3.csv'\n",
    "\n",
    "try:\n",
    "    df1 = pd.read_csv(file1_url)\n",
    "    df2 = pd.read_csv(file2_url)\n",
    "    df3 = pd.read_csv(file3_url)\n",
    "    \n",
    "    # Combine the three DataFrames\n",
    "    raw_data_combined = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "    print(f\"   Success! Combined total rows: {raw_data_combined.shape[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error reading/combining files: {e}\")\n",
    "\n",
    "\n",
    "# step 2 Standardizing Column Names\n",
    "\n",
    "def standardize_column_names(df):\n",
    "    cols = df.columns\n",
    "    # Lowercase, replace spaces/hyphens/dots with underscores, and remove residual non-word characters\n",
    "    cols = [re.sub(r'[^\\w]', '', re.sub(r'[\\s\\-\\.]', '_', col.lower())) for col in cols]\n",
    "    df.columns = cols\n",
    "    return df\n",
    "\n",
    "combined_data_clean_cols = standardize_column_names(raw_data_combined.copy())\n",
    "print(f\"   First 5 new columns: {combined_data_clean_cols.columns[:5].tolist()}\")\n",
    "\n",
    "\n",
    "# STEP 3: Cleaning Text Data (Lowercasing & Standardizing)\n",
    "\n",
    "def clean_text_data(df):\n",
    "    object_cols = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    for col in object_cols:\n",
    "        series_to_clean = df[col]\n",
    "        \n",
    "        # 1. Convert the Series values to string type.\n",
    "        series_to_clean = series_to_clean.astype(str)\n",
    "        \n",
    "        # 2. **FINAL FIX**: Use .apply(lambda x) and force str() conversion on the element 'x'\n",
    "        # This guarantees 'x' is a string before calling .lower()\n",
    "        series_to_clean = series_to_clean.apply(lambda x: str(x).lower().strip())\n",
    "        \n",
    "        # 3. Replace common string representations of NaN with actual NaN\n",
    "        series_to_clean.replace(['nan', 'na', 'n/a', '-'], np.nan, inplace=True)\n",
    "        \n",
    "        # 4. Assign the cleaned Series back to the DataFrame\n",
    "        df[col] = series_to_clean\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run the fixed function\n",
    "combined_data_clean_text = clean_text_data(combined_data_clean_cols.copy())\n",
    "print(\"Text cleaning done.\")\n",
    "\n",
    "\n",
    "## STEP 4: Converting Numeric Data Types...\n",
    "\n",
    "def clean_and_convert_numerics(df):\n",
    "    numeric_cols_to_clean = [\n",
    "        'customer_lifetime_value', \n",
    "        'income', \n",
    "        'monthly_premium_auto', \n",
    "        'total_claim_amount',\n",
    "        'number_of_open_complaints',\n",
    "        'number_of_policies'\n",
    "    ]\n",
    "    \n",
    "    for col in numeric_cols_to_clean:\n",
    "        if col in df.columns:\n",
    "            # Remove '$', ',', '%', and other symbols\n",
    "            df[col] = df[col].astype(str).str.replace(r'[$,%]', '', regex=True)\n",
    "            \n",
    "            # Convert to numeric, coercing errors to NaN\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "combined_data_clean_numerics = clean_and_convert_numerics(combined_data_clean_text.copy())\n",
    "print(f\"   Numeric columns dtypes: {combined_data_clean_numerics[['income', 'total_claim_amount']].dtypes.to_dict()}\")\n",
    "\n",
    "\n",
    "# STEP 5: Handling Missing Values (Imputation)\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "    object_cols = df.select_dtypes(include='object').columns\n",
    "    \n",
    "    # Impute Numeric Columns with the Mean\n",
    "    for col in numeric_cols:\n",
    "        df[col].fillna(df[col].mean(), inplace=True)\n",
    "        \n",
    "    # Impute Categorical Columns with the Mode\n",
    "    for col in object_cols:\n",
    "        try:\n",
    "            # Mode can fail if all values are NaN\n",
    "            mode_val = df[col].mode().iloc[0] \n",
    "            df[col].fillna(mode_val, inplace=True)\n",
    "        except IndexError:\n",
    "            # Fallback for columns entirely filled with NaNs\n",
    "            df[col].fillna('UNKNOWN', inplace=True) \n",
    "        \n",
    "    return df\n",
    "\n",
    "combined_data_final_clean = handle_missing_values(combined_data_clean_numerics.copy())\n",
    "total_missing = combined_data_final_clean.isnull().sum().sum()\n",
    "\n",
    "print(f\"   Imputation complete. Total missing values remaining: {total_missing}\")\n",
    "\n",
    "# Final verification\n",
    "print(\"Final Clean Data Sample:\")\n",
    "print(combined_data_final_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "415173ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success! Your clean data has been saved to: combined_data_final_clean.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "output_file_path = 'combined_data_final_clean.csv'\n",
    "\n",
    "\n",
    "try:\n",
    "    combined_data_final_clean.to_csv(output_file_path, index=False)\n",
    "    print(f\"✅ Success! Your clean data has been saved to: {output_file_path}\")\n",
    "\n",
    "    \n",
    "except NameError:\n",
    "    print(\"\\nERROR: 'combined_data_final_clean' is not defined.\")\n",
    "    print(\"Please ensure you run all five steps of the consolidated pipeline (with the final fix for Step 3) before running this cell.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while saving the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6902433-7a0a-4d33-b882-7b5ca24860cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce651248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac45b25-db52-4127-9b6a-97f80505d9de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31b8a9e7-7db9-4604-991b-ef6771603e57",
   "metadata": {
    "id": "31b8a9e7-7db9-4604-991b-ef6771603e57"
   },
   "source": [
    "# Challenge 2: Structuring Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a877fd6d-7a0c-46d2-9657-f25036e4ca4b",
   "metadata": {
    "id": "a877fd6d-7a0c-46d2-9657-f25036e4ca4b"
   },
   "source": [
    "In this challenge, we will continue to work with customer data from an insurance company, but we will use a dataset with more columns, called marketing_customer_analysis.csv, which can be found at the following link:\n",
    "\n",
    "https://raw.githubusercontent.com/data-bootcamp-v4/data/main/marketing_customer_analysis_clean.csv\n",
    "\n",
    "This dataset contains information such as customer demographics, policy details, vehicle information, and the customer's response to the last marketing campaign. Our goal is to explore and analyze this data by performing data cleaning, formatting, and structuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa10d9b0-1c27-4d3f-a8e4-db6ab73bfd26",
   "metadata": {
    "id": "aa10d9b0-1c27-4d3f-a8e4-db6ab73bfd26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'marketing_customer_analysis_clean.csv' loaded successfully.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- 1. Total Revenue by Sales Channel (Pivot Table) ---\n",
      "               total_claim_amount\n",
      "sales_channel                    \n",
      "Agent                  1810226.82\n",
      "Branch                 1301204.00\n",
      "Call Center             926600.82\n",
      "Web                     706600.04\n",
      "\n",
      "--- 2. Average CLV by Gender and Education (Pivot Table) ---\n",
      "education  Bachelor  College   Doctor  High School or Below   Master\n",
      "gender                                                              \n",
      "F           7874.27  7748.82  7328.51               8675.22  8157.05\n",
      "M           7703.60  8052.46  7415.33               8149.69  8168.83\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATA_URL = \"https://raw.githubusercontent.com/data-bootcamp-v4/data/main/marketing_customer_analysis_clean.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(DATA_URL)\n",
    "    print(\"'marketing_customer_analysis_clean.csv' loaded.\")\n",
    "    print(\"-\" * 50)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "\n",
    "sales_channel_revenue = pd.pivot_table(\n",
    "    df,\n",
    "    index='sales_channel',\n",
    "    values='total_claim_amount',\n",
    "    aggfunc='sum'\n",
    ")\n",
    "\n",
    "sales_channel_revenue = sales_channel_revenue.round(2)\n",
    "\n",
    "print(\"\\n--- 1. Total Revenue by Sales Channel (Pivot) ---\")\n",
    "print(sales_channel_revenue)\n",
    "\n",
    "\n",
    "clv_by_gender_education = pd.pivot_table(\n",
    "    df,\n",
    "    index='gender',\n",
    "    columns='education',\n",
    "    values='customer_lifetime_value',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "clv_by_gender_education = clv_by_gender_education.round(2)\n",
    "\n",
    "print(\"\\n--- 2. avg CLV by Gender and Education (Pivot) ---\")\n",
    "print(clv_by_gender_education)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df35fd0d-513e-4e77-867e-429da10a9cc7",
   "metadata": {
    "id": "df35fd0d-513e-4e77-867e-429da10a9cc7"
   },
   "source": [
    "1. You work at the marketing department and you want to know which sales channel brought the most sales in terms of total revenue. Using pivot, create a summary table showing the total revenue for each sales channel (branch, call center, web, and mail).\n",
    "Round the total revenue to 2 decimal points.  Analyze the resulting table to draw insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640993b2-a291-436c-a34d-a551144f8196",
   "metadata": {
    "id": "640993b2-a291-436c-a34d-a551144f8196"
   },
   "source": [
    "2. Create a pivot table that shows the average customer lifetime value per gender and education level. Analyze the resulting table to draw insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c7f2e5-3d90-43e5-be33-9781b6069198",
   "metadata": {
    "id": "32c7f2e5-3d90-43e5-be33-9781b6069198"
   },
   "source": [
    "## Bonus\n",
    "\n",
    "You work at the customer service department and you want to know which months had the highest number of complaints by policy type category. Create a summary table showing the number of complaints by policy type and month.\n",
    "Show it in a long format table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d09a8f-953c-448a-a5f8-2e5a8cca7291",
   "metadata": {
    "id": "e3d09a8f-953c-448a-a5f8-2e5a8cca7291"
   },
   "source": [
    "*In data analysis, a long format table is a way of structuring data in which each observation or measurement is stored in a separate row of the table. The key characteristic of a long format table is that each column represents a single variable, and each row represents a single observation of that variable.*\n",
    "\n",
    "*More information about long and wide format tables here: https://www.statology.org/long-vs-wide-data/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a069e0b-b400-470e-904d-d17582191be4",
   "metadata": {
    "id": "3a069e0b-b400-470e-904d-d17582191be4"
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
